# Big Data Project

This project focuses on working with big data using PySpark. It involves executing queries on RDD (Resilient Distributed Datasets) and DataFrame APIs, as well as reading and writing data in CSV and Parquet formats.

## Project Structure

The project has the following structure:

`project-code/`
- `src/`
  - `main.py`
  - `rdd_api.py`
  - `sql_parquet_api.py`
  - `csv_to_parquet.py`
  - `utils.py`
- `data/`
  - `movies.csv`
  - `ratings.csv`
  - ...
- `README.md`


- `src/`: Contains the main code modules and utilities.
- `main.py`: The main script that orchestrates the execution of queries and measures the execution times.
- `rdd_api.py`: Contains the RDD-based queries implemented using the PySpark RDD API.
- `sql_parquet_api.py`: Contains the DataFrame-based queries implemented using the PySpark SQL API and Parquet file format.
- `csv_to_parquet.py`: Converts CSV files to Parquet format.
- `utils.py`: Contains utility functions for measuring execution times and calculating statistics.
- `data/`: Directory to store the input CSV files.
- `README.md`: Documentation describing the project and its usage.

## Prerequisites

- Python 3.x
- PySpark

## Setup


